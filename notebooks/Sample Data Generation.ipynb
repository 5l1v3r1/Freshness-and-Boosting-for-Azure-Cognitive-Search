{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation for Boosting and Freshness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll generate sample dataset to test and simulate freshness.\n",
    "We'll generate two datasets into two different containers to apply different approaches\n",
    "\n",
    "We'll cover 2 approaches to handle freshness boosting using these datasets\n",
    "1. Adding a freshness value field to rank based on the value\n",
    "2. Adding multiple date fields per document type to rank based on built in datetime freshness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Config file for Azure Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For direct upload to blob please define below config file in \"notebooks\" folder\n",
    "# myconfig.py\n",
    "# stg_acct = 'StorageName'\n",
    "# stg_key = 'BlobStorageKey'\n",
    "# stg_conn = 'ConnectionString'\n",
    "# container_freshness = 'freshness'\n",
    "# container_multidatefield = 'multidatefield'\n",
    "# cognitive_services_key = 'CognitiveServicesKey'\n",
    "# cognitive_services_endpoint = 'https://westeurope.api.cognitive.microsoft.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install faker to generate names and countries\n",
    "#!pip install Faker\n",
    "\n",
    "# Install sdk for Azure Cognitive Services Text Analytics\n",
    "#!pip install azure-ai-textanalytics\n",
    "\n",
    "# Install Azure Blob Storage Client Library for Python\n",
    "#!pip install azure-storage-blob\n",
    "\n",
    "from myconfig import *\n",
    "from azure.storage.blob import generate_blob_sas, BlobSasPermissions, ContainerSasPermissions , BlobClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from faker import Faker\n",
    "\n",
    "import pandas as pd \n",
    "import json\n",
    "import random\n",
    "import time\n",
    "\n",
    "def convert_date(start, end, format, prop):\n",
    "    stime = time.mktime(time.strptime(start, format))\n",
    "    etime = time.mktime(time.strptime(end, format))\n",
    "    ptime = stime + prop * (etime - stime)\n",
    "    return time.strftime(format, time.localtime(ptime))\n",
    "\n",
    "def get_random_date(start, end, prop):\n",
    "    return convert_date(start, end, '%Y-%m-%dT%H:%M:%SZ', prop)\n",
    "\n",
    "def extend_date(jsonData):\n",
    "    freqCode = jsonData['docFrequency'] \n",
    "    jsonData['published_d'] = '1900-01-01T00:00:00Z'\n",
    "    jsonData['published_w'] = '1900-01-01T00:00:00Z'\n",
    "    jsonData['published_ah'] = '1900-01-01T00:00:00Z'\n",
    "    jsonData['published_m'] = '1900-01-01T00:00:00Z'\n",
    "    jsonData['published_y'] = '1900-01-01T00:00:00Z'\n",
    "    \n",
    "    if(freqCode ==\"Daily\"): \n",
    "        jsonData['published_d'] = jsonData['published']\n",
    "    elif(freqCode ==\"Weekly\"):\n",
    "        jsonData['published_w'] = jsonData['published']\n",
    "    elif(freqCode ==\"Bi-Weekly\"):\n",
    "        jsonData['published_ah'] = jsonData['published']\n",
    "    elif(freqCode ==\"Monthly\"):\n",
    "        jsonData['published_m'] = jsonData['published']\n",
    "    elif(freqCode ==\"Yearly\"):\n",
    "        jsonData['published_y'] = jsonData['published']\n",
    "    return jsonData\n",
    "\n",
    "def extract_keyphrases(content):\n",
    "    credential = AzureKeyCredential(cognitive_services_key)\n",
    "    endpoint=cognitive_services_endpoint\n",
    "    \n",
    "    text_analytics_client = TextAnalyticsClient(endpoint, credential)\n",
    "    documents = [\n",
    "        content\n",
    "    ]\n",
    "    response = text_analytics_client.extract_key_phrases(documents, language=\"en\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    \n",
    "    keyPhrases = []\n",
    "    for doc in result:\n",
    "        keyPhrases= keyPhrases + doc.key_phrases\n",
    "    \n",
    "    return keyPhrases\n",
    "\n",
    "def extract_entities(content):\n",
    "    credential = AzureKeyCredential(cognitive_services_key)\n",
    "    endpoint=cognitive_services_endpoint\n",
    "    \n",
    "    text_analytics_client = TextAnalyticsClient(endpoint, credential)\n",
    "    documents = [ \n",
    "        content \n",
    "    ]\n",
    "    response = text_analytics_client.recognize_entities(documents, language=\"en\")\n",
    "    result = [doc for doc in response if not doc.is_error]\n",
    "    \n",
    "    entityCategories = [\"Location\", \"Person\", \"Skill\", \"Organization\", \"Event\", \"Product\"]\n",
    "  \n",
    "    entitiesList = []\n",
    "    for doc in result:\n",
    "        for entity in doc.entities:\n",
    "            if any(entity.category in c for c in entityCategories):\n",
    "                entitiesList.append(entity.text)\n",
    "    \n",
    "    return list(set(entitiesList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import News Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>857</td>\n",
       "      <td>double eviction from big brother model caprice...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>325</td>\n",
       "      <td>dj double act revamp chart show dj duo jk and ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>1590</td>\n",
       "      <td>weak dollar hits reuters revenues at media gro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>1587</td>\n",
       "      <td>apple ipod family expands market apple has exp...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>538</td>\n",
       "      <td>santy worm makes unwelcome visit thousands of ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ArticleId                                               Text  \\\n",
       "0          1833  worldcom ex-boss launches defence lawyers defe...   \n",
       "1           154  german business confidence slides german busin...   \n",
       "2          1101  bbc poll indicates economic gloom citizens in ...   \n",
       "3          1976  lifestyle  governs mobile choice  faster  bett...   \n",
       "4           917  enron bosses in $168m payout eighteen former e...   \n",
       "...         ...                                                ...   \n",
       "1485        857  double eviction from big brother model caprice...   \n",
       "1486        325  dj double act revamp chart show dj duo jk and ...   \n",
       "1487       1590  weak dollar hits reuters revenues at media gro...   \n",
       "1488       1587  apple ipod family expands market apple has exp...   \n",
       "1489        538  santy worm makes unwelcome visit thousands of ...   \n",
       "\n",
       "           Category  \n",
       "0          business  \n",
       "1          business  \n",
       "2          business  \n",
       "3              tech  \n",
       "4          business  \n",
       "...             ...  \n",
       "1485  entertainment  \n",
       "1486  entertainment  \n",
       "1487       business  \n",
       "1488           tech  \n",
       "1489           tech  \n",
       "\n",
       "[1490 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset is downloaded from Kaggele Competition\n",
    "# https://www.kaggle.com/c/learn-ai-bbc/data?select=BBC+News+Train.csv\n",
    "data = pd.read_csv(\"../data/bbc/BBC_News_Train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'tech', 'politics', 'sport', 'entertainment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique category values \n",
    "data['Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Faker and define values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sara Salazar - Never every employee each by. Production blue course event from just upon. Quite democratic quality.\n",
      "Central certain film south. Onto message think account need.\n"
     ]
    }
   ],
   "source": [
    "# Test Faker library to generate some metadata\n",
    "fake = Faker()\n",
    "print(fake.name(),\"-\", fake.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-25T10:53:25Z\n"
     ]
    }
   ],
   "source": [
    "# Test random date between 2 dates\n",
    "print(get_random_date(\"2020-04-07T15:24:04Z\", \"2020-07-28T15:24:04Z\", random.random()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define List of document Frequency types\n",
    "freqList = ['Daily','Weekly','Bi-Weekly','Monthly','Yearly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define slice of the data for test\n",
    "#dataslice = data[0:5] # get first 5 items for test\n",
    "dataslice = data       # assign full dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate Data with Freshness Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metadate with freshness value and upload to Blob Storage\n",
    "for index, row in dataslice.iterrows():\n",
    "    jsondata = {}\n",
    "    jsondata['docFrequency'] = random.choice(freqList)\n",
    "    jsondata['published'] = get_random_date(\"2020-04-01T00:00:00Z\", \"2020-07-29T00:00:00Z\", random.random())\n",
    "    jsondata['author'] = fake.name()\n",
    "    jsondata['location'] = fake.country()\n",
    "    jsondata['articleId'] = row['ArticleId']\n",
    "    jsondata['content'] = row['Text']\n",
    "    jsondata['category'] = row['Category']\n",
    "    jsondata['keyPhrases'] = extract_keyphrases(row['Text'])\n",
    "    jsondata['entities'] = extract_entities(row['Text'])\n",
    "    print(jsondata['articleId'])\n",
    "    blob_client = BlobClient.from_connection_string(conn_str=stg_conn, container_name=container_freshness, blob_name=str(jsondata['articleId'])+ '.json')\n",
    "    blob_client.upload_blob(json.dumps(jsondata),overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate MultiDate Field with Freshness Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metadata with Multidate fields and upload to Blob Storage\n",
    "for index, row in dataslice.iterrows():\n",
    "    jsondata = {}\n",
    "    jsondata['docFrequency'] = random.choice(freqList)\n",
    "    jsondata['published'] = get_random_date(\"2020-04-01T00:00:00Z\", \"2020-07-29T00:00:00Z\", random.random())\n",
    "    jsondata['author'] = fake.name()\n",
    "    jsondata['location'] = fake.country()\n",
    "    jsondata['articleId'] = row['ArticleId']\n",
    "    jsondata['content'] = row['Text']\n",
    "    jsondata['category'] = row['Category']\n",
    "    jsondata['keyPhrases'] = extract_keyphrases(row['Text'])\n",
    "    jsondata['entities'] = extract_entities(row['Text'])\n",
    "    jsondata = extend_date(jsondata)\n",
    "    print(jsondata['articleId'])\n",
    "    blob_client = BlobClient.from_connection_string(conn_str=stg_conn, container_name=container_multidatefield, blob_name=str(jsondata['articleId'])+ '.json')\n",
    "    blob_client.upload_blob(json.dumps(jsondata),overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
